# Claim Resubmission Ingestion Pipeline

An ingestion pipeline that processes EMR claim files from multiple source formats (CSV and JSON), identifies denied claims that are eligible for resubmission based on business rules, and produces:
- A JSON list of resubmission candidates with recommended changes
- A JSON list of rejected claims with reasons and categories
- A processing metrics report

This repository includes three ways to run the pipeline:
- Command-line interface (CLI) via main.py
- REST API via FastAPI in api.py
- Orchestrated workflow via Prefect in prefect_flow.py

## Contents
- main.py — core logic and CLI entrypoint
- api.py — FastAPI service to upload a file and receive results
- prefect_flow.py — Prefect flow wrapping extraction, filtering, and saving

## Business Rules (as implemented)
A denied claim is eligible for resubmission only if ALL conditions hold:
1) status == "denied" (approved claims are excluded)
2) patient_id is present (non-empty)
3) submitted_at is more than 7 days before CURRENT_DATE (default CURRENT_DATE is 2025-07-30; window = 7 days)
4) denial_reason is evaluated:
   - Non-retryable reasons: {"authorization expired", "incorrect provider type"} → excluded
   - Retryable reasons: {"missing modifier", "incorrect npi", "prior auth required"} → eligible
   - Ambiguous reasons are decided by heuristic/LLM fallback:
     - Heuristic: {"incorrect procedure": retry, "form incomplete": retry, "not billable": do not retry}
     - Otherwise, a mock LLM classifier approves if reason contains "incomplete" or "form" (case-insensitive)

For each eligible claim, recommended_changes is generated by:
- Hardcoded mappings for common patterns (e.g., "missing modifier" → "Add appropriate modifier and resubmit")
- If no match, attempts an LLM call via OpenAI (optional)
- Final fallback returns "Investigate reason: <reason>"


## File Formats and Normalization
The pipeline supports two "source systems" with differing field names that are normalized to a common schema:
- CSV files are treated as source_system = "alpha" with fields:
  - claim_id, patient_id, procedure_code, denial_reason, status, submitted_at
- JSON files are treated as source_system = "beta" with fields mapped to:
  - id → claim_id, member → patient_id, code → procedure_code, error_msg → denial_reason, status → status, date → submitted_at

Dates are parsed from formats: YYYY-mm-dd, YYYY-mm-ddTHH:MM:SS, or mm/dd/YYYY.
Whitespace is trimmed and strings normalized to lowercase during processing.

## Requirements
You can install dependencies from the requirements.txt file.

Environment variables (with defaults in code):
- LLM_MODEL (default: gpt-4.1)
- LOG_LEVEL (default: INFO)
- OPENAI_API_KEY via environment (only required if you want to enable LLM)

## Running the CLI
Process all files in a directory (CSV treated as alpha, JSON treated as beta):

- Basic usage:
  python main.py <directory>

- With options:
  python main.py <directory> \
    -o resubmission_candidates.json \
    -r processing_report.json \
    -e rejected_claims.log \
    -re rejected_claims.json

Outputs:
- resubmission_candidates.json — eligible claims with recommended_changes
- rejected_claims.json — rejected claims with exact rejection_reason and category
- processing_report.json — run timestamp, config used, and metrics
- rejected_claims.log — errors recorded during processing


## Running the API (FastAPI)
Start the service:
- uvicorn api:app --host 0.0.0.0 --port 8000

Endpoints:
- POST /upload — multipart file upload (CSV or JSON); returns list of eligible claims
- GET  /rejected — returns the in-memory list of rejected claims from the current process
- GET  /metrics — returns current in-memory metrics

Example curl for upload:
- curl -F "file=@emr_files/emr_alpha.csv" http://localhost:8000/upload


## Running the Prefect Flow
Execute the flow locally (creates output/ directory with a timestamped JSON file):
- python prefect_flow.py

Flow graph:
- extract_claims(file_path) → filter_eligible() → load_results()


## Inputs and Outputs
Input directory can contain a mix of .csv and .json files. Each file is read and normalized; invalid or excluded claims are recorded in metrics and in rejected_claims.json with detailed reasons.

Output examples (simplified):
- Eligible claim item:
  {
    "claim_id": "A124",
    "resubmission_reason": "incorrect npi",
    "source_system": "alpha",
    "recommended_changes": "Verify provider NPI and resubmit"
  }

- Rejected claim item:
  {
    "claim_id": "A123",
    "patient_id": "P42",
    "procedure_code": "99213",
    "denial_reason": "authorization expired",
    "submitted_at": "2025-07-28",
    "source_system": "beta",
    "rejection_reason": "Non-retryable reason: authorization expired",
    "rejection_category": "non_retryable_reason"
  }


## Observability and Metrics
The processor maintains counters:
- total_claims
- resubmission_candidates
- source_counts by source_system
- exclusion_reasons with categories like invalid_data, status_approved, submitted_recently, non_retryable_reason, ambiguous_reason, unclassifiable_reason, processing_error

A processing report is saved that captures timestamp, config, and metrics.

## Error Handling
- Robust parsing for dates and formats
- File-type validation at the API boundary
- Errors during processing are logged and counted; CLI also writes errors to a file
- Atomic writes in the Prefect flow to avoid partial files

## Extending the Pipeline
- Add new EMR sources by updating normalize_field_names and process_file routing
- Append to HARDCODED_RETRYABLE and HARDCODED_NON_RETRYABLE sets as business rules evolve
- Replace mock LLM classification with a real model; ensure OPENAI_API_KEY is set and the OpenAI library is installed
- Add data persistence for API endpoints (currently in-memory)

## Local Development
- Create and activate a virtual environment
  python -m venv .venv
  source .venv/bin/activate

- Install common libs
  pip install -r requirements.txt

- Set environment variables securely
  export OPENAI_API_KEY={{OPENAI_API_KEY}}


## Alignment with Screening PDF (Acceptance Criteria)
Based on the screening document, this project implements Case Study #1 as follows:

Step 1: Schema Normalization
- Unifies source 1 (CSV, emr_alpha.csv) and source 2 (JSON, emr_beta.json) into a common schema:
  {claim_id, patient_id, procedure_code, denial_reason, status, submitted_at, source_system}
- Normalizes dates from multiple formats to ISO dates
- Handles null/empty values and standardizes casing/whitespace
- Adds source_system = alpha (CSV) or beta (JSON)

Step 2: Resubmission Eligibility Logic (assuming today = 2025-07-30)
- Requires status == denied
- Requires non-null patient_id
- Requires submitted_at more than 7 days ago
- Denial reason handling:
  - Retryable: {"Missing modifier", "Incorrect NPI", "Prior auth required"}
  - Non-retryable: {"Authorization expired", "Incorrect provider type"}
  - Ambiguous: {"incorrect procedure", "form incomplete", "not billable", null}
    - Heuristic approves retry for "incorrect procedure" and "form incomplete"; denies for "not billable"/null
    - Mock LLM fallback approves if reason contains "incomplete" or "form"

Step 3: Output
- Produces resubmission candidates with fields:
  {claim_id, resubmission_reason, source_system, recommended_changes}
- Recommended changes are derived via hardcoded mappings, LLM (optional), or a safe fallback

Metrics/Logging
- Tracks totals: total_claims, resubmission_candidates, per-source counts
- Tracks exclusion_reasons for each rejection category
- CLI writes processing_report.json and rejected_claims.log

Graceful Handling of Malformed/Missing Data
- Defensive parsing and try/except blocks to keep the pipeline running
- Rejected records are captured to rejected_claims.json with explicit reasons

Final Deliverables Checklist
- [x] Working script (main.py) performing normalization, logic, and output
- [x] Output saved to resubmission_candidates.json (CLI) and via Prefect flow to output/…json
- [x] Basic logging and metrics with a saved processing_report.json
- [x] Graceful handling of malformed/missing data

Bonus Stretch Goals Implemented
- [x] Modularized using a ClaimsProcessor class with clear responsibilities
- [x] FastAPI endpoint (api.py) for file upload and results
- [x] Prefect flow (prefect_flow.py) for orchestration
- [x] Mocked/heuristic LLM logic for ambiguous denial reasons
- [x] Export of failed records to rejected_claims.json and error log file

Sample Data (from PDF)
- Source 1 (CSV headers): claim_id,patient_id,procedure_code,denial_reason,submitted_at,status
  Example rows:
  - A123,P001,99213,Missing modifier,2025-07-01,denied
  - A124,P002,99214,Incorrect NPI,2025-07-10,denied
  - A125,,99215,Authorization expired,2025-07-05,denied
  - A126,P003,99381,None,2025-07-15,approved
  - A127,P004,99401,Prior auth required,2025-07-20,denied

- Source 2 (JSON array): elements with keys {id, member, code, error_msg, date, status}
  Example items include:
  - {"id":"B987","member":"P010","code":"99213","error_msg":"Incorrect provider type","date":"2025-07-03T00:00:00","status":"denied"}
  - {"id":"B988","member":"P011","code":"99214","error_msg":"Missing modifier","date":"2025-07-09T00:00:00","status":"denied"}
  - {"id":"B989","member":"P012","code":"99215","error_msg":null,"date":"2025-07-10T00:00:00","status":"approved"}
  - {"id":"B990","member":null,"code":"99401","error_msg":"incorrect procedure","date":"2025-07-01T00:00:00","status":"denied"}

How to Reproduce with Sample Files
- Place emr_alpha.csv and emr_beta.json in a directory, e.g., emr_files/
- Run the CLI:
  python main.py emr_files \
    -o resubmission_candidates.json \
    -r processing_report.json \
    -e rejected_claims.log \
    -re rejected_claims.json

This will produce the required outputs and metrics per the PDF.
